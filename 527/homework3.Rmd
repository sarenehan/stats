---
title: "Homework 3"
author: "Stewart Renehan"
date: "5/8/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### a)
We take the derivative of a with respect to the value we are attempting to minimize:
$$\frac{d}{da}[\lvert y - X a \rvert ^ 2 + \lambda a^T \Omega a] = - 2 X^T ( y - X a ) + 2 \lambda \Omega a$$

$$2 X^T X a + 2 \lambda \Omega a = 2 X^T y$$

$$a (X^T X + \lambda \Omega) = X^T y$$

$$\hat{a} = (X^T X + \lambda \Omega)^{-1} X^T y$$

Plugging this into

$$\hat{y} = X\hat{a}$$

gives:

$$\hat{y} = X (X^T X + \lambda \Omega)^{-1} X^T y $$


#### b)

```{r}
source('/Users/stewart/projects/stats/527/homework_3_test_data.R')
require(MASS)
truncated.power.design.matrix <- function() {
  design.matrix = matrix(0, ncol=length(x), nrow=length(x));
  design.matrix[, length(x)] = 1;
  for (i in 1:length(x) - 1) {
    design.matrix[i:length(x), i] = (x - x[i])[i:length(x)]
  }
  return(design.matrix);
}
X = truncated.power.design.matrix();
Omega = diag(length(x));
Omega[1, 1] = 0;
Omega[length(x), length(x)] = 0;
get.fitted.values <- function(lambda) {
  return(X %*% ginv((t(X) %*% X + (lambda * Omega))) %*% t(X) %*% y)
}

for (lambda in c(0, 1, 10)) {
  plot(x, get.fitted.values(lambda), ylab='Fitted Value', xlab='x', main=paste0('Fitted Values for Lambda = ', lambda))
}
plot(x, get.fitted.values(10^6), ylab='Fitted Value', xlab='x', main=paste0('Fitted Values for Lambda = ', lambda))
abline(lm(y~x))
```


The line for $\lambda = 10^6$ is equal to the regression line, as shown above.

#### c)
Plugging in the value of N to the Reinsch form and multiplying by $I = X X^{-1}$ on the left and $I = X^{-T} X^T$ gives:

$$(I + \lambda X^{-T} \Omega  X^{-1})^{-1} = X X^{-1} ( I + \lambda X^{-T} \Omega X^ {-1})^{-1} X^{-T} X^T$$

$$ = X(X^T (I + \lambda X^{-T} \Omega X^{-1}) X)^{-1} X^T$$

$$ = X(X^TX + \lambda X^T X^{-T} \Omega X^{-1} X )^{-1} X^T$$

On the right side of the inverse, the $X^T X^{-T}$ and $X^{-1} X$ both equal the identity, reducing the equation to:

$$ X (X^T X + \lambda \Omega)^{-1} X^T = S_\lambda$$

Which is the same as above.

#### d) 

$$cov(y, \hat{y}) = cov(y, S_\lambda y)$$
$$ = S_\lambda cov(y, y)$$
$$= S_\lambda \sigma ^2 $$
$$=> \sum{cov(y, \hat{y}}) = \sigma^2\sum{S_\lambda} = \sigma^2 tr(S_\lambda) = p \sigma^2$$

#### e)

$$S_\lambda = (I + \lambda N)^{-1} = (I + \lambda U D U^T)^{-1}$$

$$=UU^{-1}(I + \lambda UDU^T)^{-1}U^{-T}U^T$$
$$=U(U^T(I + \lambda UDU^T)U)^{-1}U^T$$

$$=U(U^TIU + \lambda U^TUDU^TU)^{-1}U^T$$
$$=U(I + \lambda D)^{-1}U^T$$












